
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Information Gain Computation in Python &#8212; Tutorials on Data Science with Python</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'ml/Decision_Trees_InfoGain_Computation';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Hotel Prediction with Hybrid Collaborative Filtering with SVD" href="Case_Study3_Hotel_Prediction.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Tutorials on Data Science with Python</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Welcome to D. Akman’s Tutorials on Data Science with Python!
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../python/README.html">Python Basics (PB) Tutorial Series</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../python/PB1_nb_intro.html">INTRODUCTION TO JUPYTER NOTEBOOKS</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python/PB2_nb_markdown.html">NOTEBOOK MARKDOWN TUTORIAL</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python/PB3_intro_to_python.html">Introduction to Python Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python/PB4_numpy.html">NumPy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python/PB5_pandas.html">Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python/PB6_matplotlib.html">Matplotlib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python/PB7_seaborn.html">Seaborn</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python/PB8_python_vs_r.html">Python vs. R</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../stats/README.html">Statistics Tutorials/ OpenIntro Labs for Python</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../stats/ch1n2_intro_to_data.html">Introduction to data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stats/ch3_probability.html">Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stats/ch4_normal_distribution.html">Normal distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stats/ch5_confidence_intervals.html">Foundations for statistical inference - Confidence intervals</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stats/ch5_sampling_distributions.html">Foundations for statistical inference - Sampling distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stats/ch6_inf_for_categorical_data.html">Inference for categorical data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stats/ch7_inf_for_numerical_data.html">Inference for numerical data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stats/ch8_simple_regression.html">Introduction to linear regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stats/ch9_multiple_regression.html">Multiple linear regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stats/regression_case_study_predicting_age_in_census_data.html">Predicting Age in Census Data<a class="anchor-link" href="#Predicting-Age-in-Census-Data"></a></a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="README.html">Machine Learning Tutorials with Scikit-Learn</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="Data_Prep_for_Predictive_Modelling.html">Data Preparation for Predictive Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="SK0_Scikit_Learn_Introduction.html">SK Part 0: Introduction to Predictive Modeling with Python and Scikit-Learn</a></li>
<li class="toctree-l2"><a class="reference internal" href="SK1_Basic_Modelling.html">SK Part 1: Basic Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="SK2_Feature_Selection.html">SK Part 2: Feature Selection and Ranking</a></li>
<li class="toctree-l2"><a class="reference internal" href="SK3_Model_Evaluation.html">SK Part 3: Model Evaluation</a></li>

<li class="toctree-l2"><a class="reference internal" href="SK4_HyperParameter_Tuning.html">SK Part 4: Cross-Validation and Hyper-parameter Tuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="SK5_Advanced_Topics.html">SK Part 5: Pipelines, Statistical Model Comparison, and Model Deployment</a></li>
<li class="toctree-l2"><a class="reference internal" href="SK6_Clustering.html">SK Part 6: K-Means Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="SK7_Neural_Networks.html">SK Part 7: Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="SK8_LightGBM.html">Light GBM &amp; Parameter Tuning with Optuna</a></li>
<li class="toctree-l2"><a class="reference internal" href="SK9_Forecasting.html">Forecasting Fundamentals with Python &amp; Facebook Prophet</a></li>
<li class="toctree-l2"><a class="reference internal" href="Case_Study1_Predicting_Income_Status.html">Predicting Income Status</a></li>







<li class="toctree-l2"><a class="reference internal" href="Case_Study2_Maintenance_Predictive_Modelling.html">Predicting Optimal Machine Maintenance Cycle</a></li>
<li class="toctree-l2"><a class="reference internal" href="Case_Study3_Hotel_Prediction.html">Hotel Prediction with Hybrid Collaborative Filtering with SVD</a></li>

<li class="toctree-l2 current active"><a class="current reference internal" href="#">Information Gain Computation in Python</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/ml/Decision_Trees_InfoGain_Computation.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Information Gain Computation in Python</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-simple-example">A Simple Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-vegetation-example">The Vegetation Example</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="information-gain-computation-in-python">
<h1>Information Gain Computation in Python<a class="headerlink" href="#information-gain-computation-in-python" title="Link to this heading">#</a></h1>
<p>This tutorial illustrates how impurity and information gain can be calculated in Python using the <code class="docutils literal notranslate"><span class="pre">NumPy</span></code> and <code class="docutils literal notranslate"><span class="pre">Pandas</span></code> modules for information-based machine learning. The impurity calculation methods described in here are as follows:</p>
<ul class="simple">
<li><p>Entropy</p></li>
<li><p>Gini index</p></li>
</ul>
<p>We start off with a simple example, which is followed by the Vegetation example in the “Information-Based Learning” Chapter in the textbook.</p>
<section id="a-simple-example">
<h2>A Simple Example<a class="headerlink" href="#a-simple-example" title="Link to this heading">#</a></h2>
<p>Suppose you are going out for a picnic and you are preparing a basket of some delicious fruits.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lst</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;apple&#39;</span><span class="p">]</span><span class="o">*</span><span class="mi">3</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;orange&#39;</span><span class="p">]</span><span class="o">*</span><span class="mi">2</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;banana&#39;</span><span class="p">]</span><span class="o">*</span><span class="mi">2</span>
<span class="n">fruits</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">lst</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">fruits</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0     apple
1     apple
2     apple
3    orange
4    orange
5    banana
6    banana
dtype: object
</pre></div>
</div>
</div>
</div>
<p>Here is the relative frequency of each fruit in the basket, which can be considered as the probability distribution of the fruits.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">probs</span> <span class="o">=</span> <span class="n">fruits</span><span class="o">.</span><span class="n">value_counts</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">probs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>apple     0.428571
orange    0.285714
banana    0.285714
Name: proportion, dtype: float64
</pre></div>
</div>
</div>
</div>
<p>If you like, you can define the probability distribution yourself as below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">probs_by_hand</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="o">/</span><span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="o">/</span><span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="o">/</span><span class="mi">7</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">probs_by_hand</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.42857142857142855, 0.2857142857142857, 0.2857142857142857]
</pre></div>
</div>
</div>
</div>
<p>Recall that Shannon’s model defines entropy as
$<span class="math notranslate nohighlight">\(H(x) := - \sum_{i=1}^{\ell}(P(t=i) \times \log_{2}(P(t=i))\)</span>$
The idea with entropy is that the more heterogenous and impure a feature is, the higher the entropy. Conversely, the more homogenous and pure a feature is, the lower the entropy.</p>
<p>The following calculation shows how impurity of this fruit basket can be computed using the entropy criterion.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">entropy</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span> <span class="o">*</span> <span class="n">probs</span><span class="p">)</span>
<span class="n">entropy</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>np.float64(1.5566567074628228)
</pre></div>
</div>
</div>
</div>
<p>The gini impurity index is defined as follows:
$<span class="math notranslate nohighlight">\( \mbox{Gini}(x) := 1 - \sum_{i=1}^{\ell}P(t=i)^{2}\)</span>$
The idea with Gini index is the same as in entropy in the sense that the more heterogenous and impure a feature is, the higher the Gini index.</p>
<p>A nice property of the Gini index is that it is always between 0 and 1, and this may make it easier to compare Gini indices across different features.</p>
<p>The impurity of our fruit basket using Gini index is calculated as below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gini_index</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">probs</span><span class="p">))</span>
<span class="n">gini_index</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>np.float64(0.653061224489796)
</pre></div>
</div>
</div>
</div>
<p>In comparison, let’s compute impurity of another fruit basket with 7 different fruits with equal frequency.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lst2</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;apple&#39;</span><span class="p">,</span> <span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="s1">&#39;banana&#39;</span><span class="p">,</span> <span class="s1">&#39;mango&#39;</span><span class="p">,</span> <span class="s1">&#39;blueberry&#39;</span><span class="p">,</span> <span class="s1">&#39;watermelon&#39;</span><span class="p">,</span> <span class="s1">&#39;pear&#39;</span><span class="p">]</span>
<span class="n">fruits2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">lst2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">fruits2</span><span class="p">)</span>
<span class="n">probs2</span> <span class="o">=</span> <span class="n">fruits2</span><span class="o">.</span><span class="n">value_counts</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">probs2</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0         apple
1        orange
2        banana
3         mango
4     blueberry
5    watermelon
6          pear
dtype: object
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>apple         0.142857
orange        0.142857
banana        0.142857
mango         0.142857
blueberry     0.142857
watermelon    0.142857
pear          0.142857
Name: proportion, dtype: float64
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">entropy</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">probs2</span><span class="p">)</span> <span class="o">*</span> <span class="n">probs2</span><span class="p">)</span>
<span class="n">entropy</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>np.float64(2.807354922057604)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gini_index</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">probs2</span><span class="p">))</span>
<span class="n">gini_index</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>np.float64(0.8571428571428572)
</pre></div>
</div>
</div>
</div>
<p>As expected, both entropy and Gini index of the second fruit basket is higher than those of the first fruit basket.</p>
</section>
<section id="the-vegetation-example">
<h2>The Vegetation Example<a class="headerlink" href="#the-vegetation-example" title="Link to this heading">#</a></h2>
<p>We now work out the details of the impurity calculations for the Vegetation dataset in Chapter 5 in the textbook.</p>
<p>Let’s first import the dataset from the Cloud.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">io</span>
<span class="kn">import</span> <span class="nn">requests</span>

<span class="n">url_name</span> <span class="o">=</span> <span class="s1">&#39;https://raw.githubusercontent.com/akmand/datasets/master/FMLPDA_Table4_3.csv&#39;</span>
<span class="n">url_content</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url_name</span><span class="p">,</span> <span class="n">verify</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">content</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">io</span><span class="o">.</span><span class="n">StringIO</span><span class="p">(</span><span class="n">url_content</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)))</span>
<span class="n">df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>stream</th>
      <th>slope</th>
      <th>elevation</th>
      <th>vegetation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>False</td>
      <td>steep</td>
      <td>high</td>
      <td>chapparal</td>
    </tr>
    <tr>
      <th>1</th>
      <td>True</td>
      <td>moderate</td>
      <td>low</td>
      <td>riparian</td>
    </tr>
    <tr>
      <th>2</th>
      <td>True</td>
      <td>steep</td>
      <td>medium</td>
      <td>riparian</td>
    </tr>
    <tr>
      <th>3</th>
      <td>False</td>
      <td>steep</td>
      <td>medium</td>
      <td>chapparal</td>
    </tr>
    <tr>
      <th>4</th>
      <td>False</td>
      <td>flat</td>
      <td>high</td>
      <td>conifer</td>
    </tr>
    <tr>
      <th>5</th>
      <td>True</td>
      <td>steep</td>
      <td>highest</td>
      <td>conifer</td>
    </tr>
    <tr>
      <th>6</th>
      <td>True</td>
      <td>steep</td>
      <td>high</td>
      <td>chapparal</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>For convenience, we define a function called <code class="docutils literal notranslate"><span class="pre">compute_impurity()</span></code> that calculates impurity of a feature using either entropy or gini index.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_impurity</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">impurity_criterion</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function calculates impurity of a feature.</span>
<span class="sd">    Supported impurity criteria: &#39;entropy&#39;, &#39;gini&#39;</span>
<span class="sd">    input: feature (this needs to be a Pandas series)</span>
<span class="sd">    output: feature impurity</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">feature</span><span class="o">.</span><span class="n">value_counts</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">impurity_criterion</span> <span class="o">==</span> <span class="s1">&#39;entropy&#39;</span><span class="p">:</span>
        <span class="n">impurity</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span> <span class="o">*</span> <span class="n">probs</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">impurity_criterion</span> <span class="o">==</span> <span class="s1">&#39;gini&#39;</span><span class="p">:</span>
        <span class="n">impurity</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">probs</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Unknown impurity criterion&#39;</span><span class="p">)</span>
        
    <span class="k">return</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">impurity</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>


<span class="c1"># let&#39;s do two quick examples.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;impurity using entropy:&#39;</span><span class="p">,</span> <span class="n">compute_impurity</span><span class="p">(</span><span class="n">fruits</span><span class="p">,</span> <span class="s1">&#39;entropy&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;impurity using gini index:&#39;</span><span class="p">,</span> <span class="n">compute_impurity</span><span class="p">(</span><span class="n">fruits</span><span class="p">,</span> <span class="s1">&#39;gini&#39;</span><span class="p">))</span>
<span class="c1"># how to test for an incorrect compute_impurity_criterion value:</span>
<span class="c1"># print(&#39;impurity using gini index:&#39;, compute_impurity(df[&#39;stream&#39;], &#39;foo&#39;))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>impurity using entropy: 1.557
impurity using gini index: 0.653
</pre></div>
</div>
</div>
</div>
<p>Let’s calculate entropy of the target feature “vegetation” using our new function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">target_entropy</span> <span class="o">=</span> <span class="n">compute_impurity</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;vegetation&#39;</span><span class="p">],</span> <span class="s1">&#39;entropy&#39;</span><span class="p">)</span>
<span class="n">target_entropy</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>np.float64(1.557)
</pre></div>
</div>
</div>
</div>
<p>Let’s compute the information gain for splitting based on a descriptive feature to figure out the best feature to split on. For this task, we do the following:</p>
<ol class="arabic simple">
<li><p>Compute impurity of the target feature (using either entropy or gini index).</p></li>
<li><p>Partition the dataset based on unique values of the descriptive feature.</p></li>
<li><p>Compute impurity for each partition.</p></li>
<li><p>Compute the remaining impurity as the weighted sum of impurity of each partition.</p></li>
<li><p>Compute the information gain as the difference between the impurity of the target feature and the remaining impurity.</p></li>
</ol>
<p>We will define another function to achieve this, called <code class="docutils literal notranslate"><span class="pre">comp_feature_information_gain()</span></code>.</p>
<p>As an example, let’s have a look at the levels of the “elevation” feature.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;elevation&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>elevation
high       3
medium     2
low        1
highest    1
Name: count, dtype: int64
</pre></div>
</div>
</div>
</div>
<p>Let’s see how the partitions look like for this feature and what the corresponding calculations are using the entropy split criterion.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">level</span> <span class="ow">in</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;elevation&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;level name:&#39;</span><span class="p">,</span> <span class="n">level</span><span class="p">)</span>
    <span class="n">df_feature_level</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;elevation&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">level</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;corresponding data partition:&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">df_feature_level</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;partition target feature impurity:&#39;</span><span class="p">,</span> <span class="n">compute_impurity</span><span class="p">(</span><span class="n">df_feature_level</span><span class="p">[</span><span class="s1">&#39;vegetation&#39;</span><span class="p">],</span> <span class="s1">&#39;entropy&#39;</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;partition weight:&#39;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_feature_level</span><span class="p">))</span> <span class="o">+</span> <span class="s1">&#39;/&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;====================&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>level name: high
corresponding data partition:
   stream  slope elevation vegetation
0   False  steep      high  chapparal
4   False   flat      high    conifer
6    True  steep      high  chapparal
partition target feature impurity: 0.918
partition weight: 3/7
====================
level name: low
corresponding data partition:
   stream     slope elevation vegetation
1    True  moderate       low   riparian
partition target feature impurity: -0.0
partition weight: 1/7
====================
level name: medium
corresponding data partition:
   stream  slope elevation vegetation
2    True  steep    medium   riparian
3   False  steep    medium  chapparal
partition target feature impurity: 1.0
partition weight: 2/7
====================
level name: highest
corresponding data partition:
   stream  slope elevation vegetation
5    True  steep   highest    conifer
partition target feature impurity: -0.0
partition weight: 1/7
====================
</pre></div>
</div>
</div>
</div>
<p>The idea here is that, for each one of the 4 data partitions above,</p>
<ol class="arabic simple">
<li><p>We compute their impurity with respect to the target feature as a stand-alone dataset.</p></li>
<li><p>We weigh these impurities with the relative number of observations in each partition. The relative number of observations is calculated as the number of observations in the partition divided by the total number of observations in the entire dataset. For instance, the weight of the first partition is 3/7.</p></li>
<li><p>We add up these weighted impurities and call it the remaining impurity for this feature.</p></li>
</ol>
<p>For instance, remaining impurity as measured by entropy for the elevation feature is 0.918 x (3/7) + 1.0 x (2/7) = 0.679.</p>
<p>Information gain is then calculated as 1.557 - 0.679 = 0.878.</p>
<p>Now we are ready to define our function. There is a bit of coding in here, but we can assure you that trying to figure out how things work in here will be rewarding to improve your Python programming skills.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">comp_feature_information_gain</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">descriptive_feature</span><span class="p">,</span> <span class="n">split_criterion</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function calculates information gain for splitting on </span>
<span class="sd">    a particular descriptive feature for a given dataset</span>
<span class="sd">    and a given impurity criteria.</span>
<span class="sd">    Supported split criterion: &#39;entropy&#39;, &#39;gini&#39;</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;target feature:&#39;</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;descriptive_feature:&#39;</span><span class="p">,</span> <span class="n">descriptive_feature</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;split criterion:&#39;</span><span class="p">,</span> <span class="n">split_criterion</span><span class="p">)</span>
            
    <span class="n">target_entropy</span> <span class="o">=</span> <span class="n">compute_impurity</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">target</span><span class="p">],</span> <span class="n">split_criterion</span><span class="p">)</span>

    <span class="c1"># we define two lists below:</span>
    <span class="c1"># entropy_list to store the entropy of each partition</span>
    <span class="c1"># weight_list to store the relative number of observations in each partition</span>
    <span class="n">entropy_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="n">weight_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    
    <span class="c1"># loop over each level of the descriptive feature</span>
    <span class="c1"># to partition the dataset with respect to that level</span>
    <span class="c1"># and compute the entropy and the weight of the level&#39;s partition</span>
    <span class="k">for</span> <span class="n">level</span> <span class="ow">in</span> <span class="n">df</span><span class="p">[</span><span class="n">descriptive_feature</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">():</span>
        <span class="n">df_feature_level</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="n">descriptive_feature</span><span class="p">]</span> <span class="o">==</span> <span class="n">level</span><span class="p">]</span>
        <span class="n">entropy_level</span> <span class="o">=</span> <span class="n">compute_impurity</span><span class="p">(</span><span class="n">df_feature_level</span><span class="p">[</span><span class="n">target</span><span class="p">],</span> <span class="n">split_criterion</span><span class="p">)</span>
        <span class="n">entropy_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">entropy_level</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        <span class="n">weight_level</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">df_feature_level</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
        <span class="n">weight_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">weight_level</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;impurity of partitions:&#39;</span><span class="p">,</span> <span class="n">entropy_list</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;weights of partitions:&#39;</span><span class="p">,</span> <span class="n">weight_list</span><span class="p">)</span>

    <span class="n">feature_remaining_impurity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">entropy_list</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">weight_list</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;remaining impurity:&#39;</span><span class="p">,</span> <span class="n">feature_remaining_impurity</span><span class="p">)</span>
    
    <span class="n">information_gain</span> <span class="o">=</span> <span class="n">target_entropy</span> <span class="o">-</span> <span class="n">feature_remaining_impurity</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;information gain:&#39;</span><span class="p">,</span> <span class="n">information_gain</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;====================&#39;</span><span class="p">)</span>

    <span class="k">return</span><span class="p">(</span><span class="n">information_gain</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now that our function has been defined, we will call it for each descriptive feature in the dataset. First let’s call it using the entropy split criteria.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">split_criterion</span> <span class="o">=</span> <span class="s1">&#39;entropy&#39;</span>
<span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s1">&#39;vegetation&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="n">feature_info_gain</span> <span class="o">=</span> <span class="n">comp_feature_information_gain</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s1">&#39;vegetation&#39;</span><span class="p">,</span> <span class="n">feature</span><span class="p">,</span> <span class="n">split_criterion</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>target feature: vegetation
descriptive_feature: stream
split criterion: entropy
impurity of partitions: [np.float64(0.918), np.float64(1.5)]
weights of partitions: [0.429, 0.571]
remaining impurity: 1.250322
information gain: 0.306678
====================
target feature: vegetation
descriptive_feature: slope
split criterion: entropy
impurity of partitions: [np.float64(1.371), np.float64(-0.0), np.float64(-0.0)]
weights of partitions: [0.714, 0.143, 0.143]
remaining impurity: 0.9788939999999999
information gain: 0.578106
====================
target feature: vegetation
descriptive_feature: elevation
split criterion: entropy
impurity of partitions: [np.float64(0.918), np.float64(-0.0), np.float64(1.0), np.float64(-0.0)]
weights of partitions: [0.429, 0.143, 0.286, 0.143]
remaining impurity: 0.6798219999999999
information gain: 0.877178
====================
</pre></div>
</div>
</div>
</div>
<p>Now let’s call it using the gini index split criteria.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">split_criteria</span> <span class="o">=</span> <span class="s1">&#39;gini&#39;</span>
<span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s1">&#39;vegetation&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="n">feature_info_gain</span> <span class="o">=</span> <span class="n">comp_feature_information_gain</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s1">&#39;vegetation&#39;</span><span class="p">,</span> <span class="n">feature</span><span class="p">,</span> <span class="n">split_criteria</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>target feature: vegetation
descriptive_feature: stream
split criterion: gini
impurity of partitions: [np.float64(0.444), np.float64(0.625)]
weights of partitions: [0.429, 0.571]
remaining impurity: 0.5473509999999999
information gain: 0.1056490000000001
====================
target feature: vegetation
descriptive_feature: slope
split criterion: gini
impurity of partitions: [np.float64(0.56), np.float64(0.0), np.float64(0.0)]
weights of partitions: [0.714, 0.143, 0.143]
remaining impurity: 0.39984000000000003
information gain: 0.25316
====================
target feature: vegetation
descriptive_feature: elevation
split criterion: gini
impurity of partitions: [np.float64(0.444), np.float64(0.0), np.float64(0.5), np.float64(0.0)]
weights of partitions: [0.429, 0.143, 0.286, 0.143]
remaining impurity: 0.333476
information gain: 0.31952400000000003
====================
</pre></div>
</div>
</div>
</div>
<p>We observe that, with both the entropy and gini index split criteria, the highest information gain occurs with the “elevation” feature.</p>
<p>This is the for the split at the root node of the corresponding decision tree. In subsequent splits, the above procedure is repeated with the subset of the entire dataset in the current branch until the termination condition is reached.</p>
<hr class="docutils" />
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./ml"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Case_Study3_Hotel_Prediction.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Hotel Prediction with Hybrid Collaborative Filtering with SVD</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-simple-example">A Simple Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-vegetation-example">The Vegetation Example</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By D. Akman
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>